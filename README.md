# MusicEmotionRec
Music Emotion Recognition using RNNs for Data Mining Research Project.

This study explores the application of neural network technologies to recognize emotions conveyed
in music, aiming to enhance music recommendation systems and support therapeutic interventions by
tailoring music to fit listeners’ emotional states. We utilize Russell’s Emotion Quadrant to categorize
music into four distinct emotional regions and develop models capable of accurately predicting these
categories. Our approach involves extracting a comprehensive set of audio features using Librosa
and applying various recurrent neural network architectures, including standard RNNs, Bidirectional
RNNs, and Long Short-Term Memory (LSTM) networks. Initial experiments are conducted using a
dataset of 900 audio clips, labeled according to the emotional quadrants. We compare the performance
of our neural network models against a set of baseline classifiers and analyze their effectiveness in
capturing the temporal dynamics inherent in musical expression. The results indicate that simpler
RNN architectures may perform comparably or even superiorly to more complex models, particularly
in smaller datasets. This research not only enhances our understanding of the emotional impact
of music but also demonstrates the potential of neural networks in creating more personalized and
emotionally resonant music recommendation and therapy systems.

## Datasets

900 30-sec Audio Clips with 4Q Labels(Russell's Emotion Quadrant):
https://mir.dei.uc.pt/downloads.html

14,000 Audio Dataset (mtg-jamendo-dataset):
https://github.com/MTG/mtg-jamendo-dataset#downloading-the-dataset

## Related Works

Paper Reports: [Need Update]

Code documents: Included in this Github Repo.
